{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine data set\n",
    "# Characteristics: 11 features, 4898 samples, 11 classes (0-10)\n",
    "\n",
    "# Load the data set\n",
    "wine = np.loadtxt('datasets/winequality/winequality-white.csv', delimiter=';', skiprows=1)\n",
    "\n",
    "# extract the data and labels\n",
    "wine_data = []\n",
    "wine_labels = []\n",
    "\n",
    "for row in wine:\n",
    "    wine_data.append(row[:-1])\n",
    "    wine_labels.append(row[-1])\n",
    "\n",
    "# convert to numpy arrays\n",
    "wine_data = np.array(wine_data)\n",
    "wine_labels = np.array(wine_labels)\n",
    "\n",
    "wine_possible_labels = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HAR data set\n",
    "# Characteristics: 561 features, 10299 samples, 6 classes (1-6)\n",
    "\n",
    "# Load the files\n",
    "X_test = np.loadtxt('datasets/UCI HAR Dataset/test/X_test.txt')\n",
    "y_test = np.loadtxt('datasets/UCI HAR Dataset/test/y_test.txt')\n",
    "X_train = np.loadtxt('datasets/UCI HAR Dataset/train/X_train.txt')\n",
    "y_train = np.loadtxt('datasets/UCI HAR Dataset/train/y_train.txt')\n",
    "\n",
    "# format the data set so that all of this data is in one data set due to the given charactersitics\n",
    "har_data = np.concatenate((X_test, X_train), axis=0)\n",
    "har_labels = np.concatenate((y_test, y_train), axis=0)\n",
    "\n",
    "har_possible_labels = np.array([1, 2, 3, 4, 5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement minimum-probability-of-error classifier, assuming the class conditional pdfs are Gaussian\n",
    "# using all available samples from a class, with sample averages, estimate mean vectors and covariance matrices\n",
    "# using sample counts, also estimate class priors\n",
    "\n",
    "# calculate mean vectors, covariance matrices, and priors for each class\n",
    "# also return the unique labels\n",
    "def calculate_parameters(data: np.array, labels: np.array) -> tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Calculates the mean vectors, covariance matrices, and priors for each class.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): The data set.\n",
    "        labels (np.array): The labels for the data set.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, np.array, np.array, np.array]: The unique labels, mean vectors, covariance matrices, and priors.\n",
    "    \"\"\"\n",
    "    # get the unique labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # calculate the mean vectors\n",
    "    mean_vectors = []\n",
    "    for label in unique_labels:\n",
    "        mean_vectors.append(np.mean(data[labels == label], axis=0))\n",
    "\n",
    "    # calculate the covariance matrices\n",
    "    covariance_matrices = []\n",
    "    for label in unique_labels:\n",
    "        covariance_matrices.append(np.cov(data[labels == label].T))\n",
    "\n",
    "    # calculate the priors\n",
    "    priors = []\n",
    "    for label in unique_labels:\n",
    "        priors.append(np.sum(labels == label) / len(labels))\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    mean_vectors = np.array(mean_vectors)\n",
    "    covariance_matrices = np.array(covariance_matrices)\n",
    "    priors = np.array(priors)\n",
    "\n",
    "    return unique_labels, mean_vectors, covariance_matrices, priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean vector and covariance matrix for wine data set\n",
    "wine_unique_labels, wine_mean_vectors, wine_covariance_matrices, wine_priors = calculate_parameters(wine_data, wine_labels)\n",
    "\n",
    "# mean vector and covariance matrix for HAR data set\n",
    "har_unique_labels, har_mean_vectors, har_covariance_matrices, har_priors = calculate_parameters(har_data, har_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a regularization term to the covariance matrices to ensure the regularized covaraince matrix has all eigenvalues larger than 0\n",
    "# this is done by adding a small value to the diagonal of the covariance matrix\n",
    "# for now, we'll use a value on the order of arithmetic average of sample covariance matrices\n",
    "def regularize_covariance_matrices(covariance_matrices: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Regularizes the covariance matrices.\n",
    "\n",
    "    Args:\n",
    "        covariance_matrices (np.array): The covariance matrices.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The regularized covariance matrices.\n",
    "    \"\"\"\n",
    "    # calculate the average covariance matrix\n",
    "    average_covariance_matrix = np.mean(covariance_matrices, axis=0)\n",
    "\n",
    "    # calculate the regularization term\n",
    "    regularization_term = np.mean(np.diag(average_covariance_matrix))\n",
    "\n",
    "    print(\"The regularization term is: \", regularization_term)\n",
    "\n",
    "    # add the regularization term to the covariance matrices\n",
    "    for i in range(len(covariance_matrices)):\n",
    "        covariance_matrices[i] += regularization_term * np.eye(covariance_matrices[i].shape[0])\n",
    "\n",
    "    return covariance_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularization term is:  353.1966224105839\n",
      "The regularization term is:  0.03549356441569865\n"
     ]
    }
   ],
   "source": [
    "# add the regularization term to the covariance matrices\n",
    "wine_covariance_matrices = regularize_covariance_matrices(wine_covariance_matrices)\n",
    "har_covariance_matrices = regularize_covariance_matrices(har_covariance_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the minimum-probability-of-error classifier\n",
    "def minimum_probability_of_error_classifier(x: np.array, unique_labels: np.array, mean_vectors: np.array, covariance_matrices: np.array, priors: np.array) -> int:\n",
    "    \"\"\"\n",
    "    Implements the minimum-probability-of-error classifier.\n",
    "\n",
    "    Args:\n",
    "        x (np.array): The data point to classify.\n",
    "        unique_labels (np.array): The unique, used labels for the data set.\n",
    "        mean_vectors (np.array): The mean vectors for each class.\n",
    "        covariance_matrices (np.array): The covariance matrices for each class.\n",
    "        priors (np.array): The priors for each class.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted label.\n",
    "    \"\"\"\n",
    "    # initialize the probabilities\n",
    "    probabilities = []\n",
    "\n",
    "    # iterate through the classes\n",
    "    for i in range(len(unique_labels)):\n",
    "        # calculate the probability\n",
    "        probability = multivariate_normal.pdf(x, mean_vectors[i], covariance_matrices[i]) * priors[i]\n",
    "\n",
    "        # add the probability to the list\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    # find the maximum probability\n",
    "    max_probability = max(probabilities)\n",
    "\n",
    "    # find the index of the maximum probability\n",
    "    max_probability_index = probabilities.index(max_probability)\n",
    "\n",
    "    # use the max_probability_index to find the predicted label\n",
    "    predicted_label = unique_labels[max_probability_index]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "# implement a function that will classify a data set using the minimum-probability-of-error classifier\n",
    "def classify_data_set(data: np.array, unique_labels: np.array, mean_vectors: np.array, covariance_matrices: np.array, priors: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Classifies a data set using the minimum-probability-of-error classifier.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): The data set to classify.\n",
    "        unique_labels (np.array): The unique, used labels for the data set.\n",
    "        mean_vectors (np.array): The mean vectors for each class.\n",
    "        covariance_matrices (np.array): The covariance matrices for each class.\n",
    "        priors (np.array): The priors for each class.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The predicted labels.\n",
    "    \"\"\"\n",
    "    # initialize the predicted labels\n",
    "    predicted_labels = []\n",
    "\n",
    "    # iterate through the data set\n",
    "    for i in range(len(data)):\n",
    "        # classify the data point\n",
    "        predicted_label = minimum_probability_of_error_classifier(data[i], unique_labels, mean_vectors, covariance_matrices, priors)\n",
    "\n",
    "        # add the predicted label to the list\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # convert to a numpy array\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# implement a function that will count the errors, the error probability estimate, and the confusion matrix\n",
    "def calculate_classification_metrics(predicted_labels: np.array, actual_labels: np.array, possible_labels: np.array) -> tuple[int, float, np.array]:\n",
    "    \"\"\"\n",
    "    Calculates the number of errors, the error probability estimate, and the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        predicted_labels (np.array): The predicted labels.\n",
    "        actual_labels (np.array): The actual labels.\n",
    "        possible_labels (np.array): All the possible labels for the data set.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, float, np.array]: The number of errors, the error probability estimate, and the confusion matrix.\n",
    "    \"\"\"\n",
    "    # initialize the number of errors\n",
    "    number_of_errors = 0\n",
    "\n",
    "    # initialize the confusion matrix\n",
    "    confusion_matrix = np.zeros((len(possible_labels), len(possible_labels)))\n",
    "\n",
    "    # iterate through the predicted labels\n",
    "    for i in range(len(predicted_labels)):\n",
    "        # check if the predicted label is correct\n",
    "        if predicted_labels[i] != actual_labels[i]:\n",
    "            # increment the number of errors\n",
    "            number_of_errors += 1\n",
    "\n",
    "        # increment the confusion matrix\n",
    "        actual_index = np.where(possible_labels == actual_labels[i])[0][0]\n",
    "        predicted_index = np.where(possible_labels == predicted_labels[i])[0][0]\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # calculate the error probability estimate\n",
    "    error_probability_estimate = number_of_errors / len(predicted_labels)\n",
    "\n",
    "    return number_of_errors, error_probability_estimate, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the wine data set\n",
    "wine_predicted_labels = classify_data_set(wine_data, wine_unique_labels, wine_mean_vectors, wine_covariance_matrices, wine_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of errors for the wine data set is:  2683\n",
      "The error probability estimate for the wine data set is:  0.5477746018783177\n",
      "The confusion matrix for the wine data set is:\n",
      "[[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 3.000e+00 0.000e+00 3.000e+00 1.400e+01\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 7.000e+00 1.550e+02\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 2.000e+00 0.000e+00 1.620e+02 1.293e+03\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.480e+02 2.050e+03\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.700e+01 8.630e+02\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 6.000e+00 1.690e+02\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the classification metrics for the wine data set\n",
    "wine_number_of_errors, wine_error_probability_estimate, wine_confusion_matrix = calculate_classification_metrics(wine_predicted_labels, wine_labels, wine_possible_labels)\n",
    "\n",
    "# print the classification metrics for the wine data set\n",
    "print(\"The number of errors for the wine data set is: \", wine_number_of_errors)\n",
    "print(\"The error probability estimate for the wine data set is: \", wine_error_probability_estimate)\n",
    "print(\"The confusion matrix for the wine data set is:\")\n",
    "print(wine_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# classify the HAR data set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m har_predicted_labels \u001b[39m=\u001b[39m classify_data_set(har_data, har_unique_labels, har_mean_vectors, har_covariance_matrices, har_priors)\n",
      "Cell \u001b[0;32mIn[103], line 60\u001b[0m, in \u001b[0;36mclassify_data_set\u001b[0;34m(data, unique_labels, mean_vectors, covariance_matrices, priors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m# iterate through the data set\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[1;32m     59\u001b[0m     \u001b[39m# classify the data point\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     predicted_label \u001b[39m=\u001b[39m minimum_probability_of_error_classifier(data[i], unique_labels, mean_vectors, covariance_matrices, priors)\n\u001b[1;32m     62\u001b[0m     \u001b[39m# add the predicted label to the list\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     predicted_labels\u001b[39m.\u001b[39mappend(predicted_label)\n",
      "Cell \u001b[0;32mIn[103], line 22\u001b[0m, in \u001b[0;36mminimum_probability_of_error_classifier\u001b[0;34m(x, unique_labels, mean_vectors, covariance_matrices, priors)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# iterate through the classes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unique_labels)):\n\u001b[1;32m     21\u001b[0m     \u001b[39m# calculate the probability\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     probability \u001b[39m=\u001b[39m multivariate_normal\u001b[39m.\u001b[39;49mpdf(x, mean_vectors[i], covariance_matrices[i]) \u001b[39m*\u001b[39m priors[i]\n\u001b[1;32m     24\u001b[0m     \u001b[39m# add the probability to the list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     probabilities\u001b[39m.\u001b[39mappend(probability)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/scipy/stats/_multivariate.py:580\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpdf\u001b[39m(\u001b[39mself\u001b[39m, x, mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cov\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, allow_singular\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    562\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Multivariate normal probability density function.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \n\u001b[1;32m    564\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \n\u001b[1;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_parameters(mean, cov, allow_singular)\n\u001b[1;32m    581\u001b[0m     dim, mean, cov_object \u001b[39m=\u001b[39m params\n\u001b[1;32m    582\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_quantiles(x, dim)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/scipy/stats/_multivariate.py:417\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters\u001b[0;34m(self, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    410\u001b[0m dim, mean, cov \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_parameters_psd(\u001b[39mNone\u001b[39;00m, mean, cov)\n\u001b[1;32m    411\u001b[0m \u001b[39m# After input validation, some methods then processed the arrays\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m# with a `_PSD` object and used that to perform computation.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39m# To avoid branching statements in each method depending on whether\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39m# `cov` is an array or `Covariance` object, we always process the\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m# array with `_PSD`, and then use wrapper that satisfies the\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39m# `Covariance` interface, `CovViaPSD`.\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m psd \u001b[39m=\u001b[39m _PSD(cov, allow_singular\u001b[39m=\u001b[39;49mallow_singular)\n\u001b[1;32m    418\u001b[0m cov_object \u001b[39m=\u001b[39m _covariance\u001b[39m.\u001b[39mCovViaPSD(psd)\n\u001b[1;32m    419\u001b[0m \u001b[39mreturn\u001b[39;00m dim, mean, cov_object\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/scipy/stats/_multivariate.py:162\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_M \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(M)\n\u001b[1;32m    159\u001b[0m \u001b[39m# Compute the symmetric eigendecomposition.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# Note that eigh takes care of array conversion, chkfinite,\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# and assertion that the matrix is square.\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m s, u \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(M, lower\u001b[39m=\u001b[39;49mlower, check_finite\u001b[39m=\u001b[39;49mcheck_finite)\n\u001b[1;32m    164\u001b[0m eps \u001b[39m=\u001b[39m _eigvalsh_to_eps(s, cond, rcond)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmin(s) \u001b[39m<\u001b[39m \u001b[39m-\u001b[39meps:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/scipy/linalg/_decomp.py:561\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    558\u001b[0m         lwork_args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlwork\u001b[39m\u001b[39m'\u001b[39m: lw}\n\u001b[1;32m    560\u001b[0m     drv_args\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mlower\u001b[39m\u001b[39m'\u001b[39m: lower, \u001b[39m'\u001b[39m\u001b[39mcompute_v\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m _job \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m})\n\u001b[0;32m--> 561\u001b[0m     w, v, \u001b[39m*\u001b[39mother_args, info \u001b[39m=\u001b[39m drv(a\u001b[39m=\u001b[39;49ma1, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdrv_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlwork_args)\n\u001b[1;32m    563\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Generalized problem\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[39m# 'gvd' doesn't have lwork query\u001b[39;00m\n\u001b[1;32m    565\u001b[0m     \u001b[39mif\u001b[39;00m driver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgvd\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classify the HAR data set\n",
    "har_predicted_labels = classify_data_set(har_data, har_unique_labels, har_mean_vectors, har_covariance_matrices, har_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the classification metrics for the HAR data set\n",
    "har_number_of_errors, har_error_probability_estimate, har_confusion_matrix = calculate_classification_metrics(har_predicted_labels, har_labels, har_possible_labels)\n",
    "\n",
    "# print the classification metrics for the HAR data set\n",
    "print(\"The number of errors for the HAR data set is: \", har_number_of_errors)\n",
    "print(\"The error probability estimate for the HAR data set is: \", har_error_probability_estimate)\n",
    "print(\"The confusion matrix for the HAR data set is:\")\n",
    "print(har_confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add some sort of percentage completion for classification\n",
    "# TODO: pretty print the confusion matrices\n",
    "# TODO: visualize the data sets in various 2 or 3 dimensional projections\n",
    "# TODO: discuss if gaussian class conditional densities are appropriate for the data sets\n",
    "# TODO: discuss how model choice influences confusion matrix and probability of error\n",
    "# TODO: explain modeling assumptions, how estimated/selected parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
